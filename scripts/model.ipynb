{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546ee328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 9.5 MB/s            \n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.6.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a741630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a541a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metricUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-320603bd00ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmetricUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metricUtils'"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import json \n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from metricUtils import tn, fp, tp, fn, precision, recall, fpr, tpr, tnr, f1, auc, mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79983e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29248e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all Json files in one Json file\n",
    "def merge_json_files(path):\n",
    "    result = []\n",
    "    for f in glob.glob(path+\"/*.json\"):\n",
    "        with open(f, \"rb\") as infile:\n",
    "            result.append(json.load(infile))\n",
    "#     print(result)\n",
    "    with open(path+\"/merged_file.json\", \"w\") as outfile:\n",
    "         json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcfa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../data/repos_'\n",
    "merge_json_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d01d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "# Metric inspired by https://towardsdatascience.com/metrics-for-imbalanced-classification-41c71549bbb5\n",
    "# Change parameters below (K and nbTree)\n",
    "\n",
    "finalScores = []\n",
    "\n",
    "def main():\n",
    "    # Checks\n",
    "#     checkUsage()\n",
    "\n",
    "    # Parameters\n",
    "    k = 10\n",
    "    nbTrees = 100\n",
    "    modes = [\"count\"] # [\"binary\", \"count\", \"tfidf\", \"freq\"]\n",
    "    numWords = [100, 2000]\n",
    "    lowerStates = [True] # [True, False]\n",
    "    cuts = [0] # [0, 4]\n",
    "\n",
    "    # Load Data\n",
    "    datasetPath = '../data/repos_/merged_file.json'\n",
    "    data = pd.read_json(datasetPath)\n",
    " \n",
    "    print(\"Data length: \", len(data))\n",
    "  \n",
    "    for i in tqdm(range(0, len(modes))):\n",
    "        mode = modes[i]\n",
    "        for numWord in numWords:\n",
    "            for lowerState in lowerStates:\n",
    "                for cut in cuts:\n",
    "                    # Shuffle Data\n",
    "                    data = shuffle(data)\n",
    "                    if cut == 0:\n",
    "                        body = data['Body'].values\n",
    "#                         body = data['Body']\n",
    "                      \n",
    "                    if cut == 2:\n",
    "                        body = data['Body'].values + data['CUT_1'].values + data['CUT_2'].values \n",
    "                    if cut == 4:\n",
    "                        body = data['Body'].values + data['CUT_1'].values + data['CUT_2'].values + data['CUT_3'].values + data['CUT_4'].values\n",
    "                    \n",
    "                    # Building Tokenizer and Vocabulary\n",
    "                    tokenizer = Tokenizer(lower=lowerState, num_words=numWord, filters='\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "                    tokenizer.fit_on_texts(body)\n",
    "                    \n",
    "                    print(\"Vocabulary size: \", len(tokenizer.word_index) + 1)\n",
    "                    print(\"Most important words:\")\n",
    "                    print(list(tokenizer.word_index.keys())[:10], \"\\n\")\n",
    "                  \n",
    "                   \n",
    "\n",
    "#                     # Random Forest Model\n",
    "#                     classifierKFold = RandomForestClassifier(n_estimators = nbTrees, random_state = 0) \n",
    "#                     X = tokenizer.texts_to_matrix(body, mode=mode)\n",
    "#                     y = data['Label'].values\n",
    "\n",
    "#                     # Cross validation, K = 10, using stratified folds\n",
    "#                     scoring = {\n",
    "#                         'precision': make_scorer(precision), \n",
    "#                         'recall': make_scorer(recall), \n",
    "#                         'f1': make_scorer(f1), \n",
    "#                         'auc': make_scorer(auc), \n",
    "#                         'fpr': make_scorer(fpr), \n",
    "#                         'tnr': make_scorer(tnr), \n",
    "#                         'mcc': make_scorer(mcc)\n",
    "#                     }\n",
    "\n",
    "#                     scores = cross_validate(classifierKFold, X, y, cv=k, scoring=scoring, verbose=0, n_jobs=10)\n",
    "                    \n",
    "#                     # Display results\n",
    "#                     # print(\"\\nMetrics\")\n",
    "#                     # displayScores(scores['test_precision'], \"Precision\")\n",
    "#                     # displayScores(scores['test_recall'], \"Recall\")\n",
    "#                     # displayScores(scores['test_f1'], \"F1\")\n",
    "#                     # displayScores(scores['test_auc'], \"AUC\")\n",
    "#                     # displayScores(scores['test_mcc'], \"MCC\")\n",
    "\n",
    "#                     # Save score\n",
    "#                     o = {\n",
    "#                         \"variables\": {\n",
    "#                             \"mode\": mode,\n",
    "#                             \"numWord\": numWord,\n",
    "#                             \"lowerState\": lowerState,\n",
    "#                             \"cut\": cut,\n",
    "#                         },\n",
    "#                         \"Precision\": round(np.nanmean(scores['test_precision']), 2),\n",
    "#                         \"Recall\": round(np.nanmean(scores['test_recall']), 2),\n",
    "#                         \"F1\": round(np.nanmean(scores['test_f1']), 2),\n",
    "#                         \"AUC\": round(np.nanmean(scores['test_auc']), 2),\n",
    "#                         \"MCC\": round(np.nanmean(scores['test_mcc']), 2)\n",
    "#                     }\n",
    "#                     finalScores.append(o)\n",
    "\n",
    "#     # Display final scores\n",
    "#     sortedScores = sorted(finalScores, key=lambda x: x[\"F1\"], reverse=True)\n",
    "\n",
    "#     # Best / Worst config\n",
    "#     print(\"\\nBest configuration:\")\n",
    "#     pprint(sortedScores[:1])\n",
    "#     print(\"\\nWorst configuration:\")\n",
    "#     pprint(sortedScores[-1:])\n",
    "\n",
    "#     # All config\n",
    "#     # pprint(sortedScores)\n",
    "\n",
    "def displayScores(scores, title):\n",
    "    print(\"\\n\",title,\":\", sep=\"\")\n",
    "    #print(\"Scores: \", scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.nanmean(scores), np.nanstd(scores) * 2))\n",
    "\n",
    "\n",
    "def checkUsage():\n",
    "    #Check the programs' arguments\n",
    "    if len(sys.argv) != 2 or not os.path.isfile(sys.argv[1]):\n",
    "        print(\"Usage: python3 model.py [path/to/dataset.json]\")\n",
    "        sys.exit(1)\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d34a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 312.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length:  9\n",
      "Vocabulary size:  27\n",
      "Most important words:\n",
      "['self', 'assertequal', 'qb', 'player', '4', 'quarterback', 'name', 'yards', 'sqrt', '2'] \n",
      "\n",
      "Vocabulary size:  27\n",
      "Most important words:\n",
      "['self', 'assertequal', 'player', 'qb', 'name', 'yards', 'quarterback', '4', 'pass', '150'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
