{
    "functionName": "test_features_hashing",
    "className": null,
    "fileName": "/FreeDiscovery_&_FreeDiscovery/freediscovery_&_engine_&_tests_&_test_integration.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "cache_dir = check_cache()\nn_features = 20000\nfe = FeatureVectorizer(cache_dir=cache_dir, mode='w')\nuuid = fe.setup(n_features=n_features, use_hashing=use_hashing)\nfe.ingest(data_dir, file_pattern='.*\\\\d.txt')\nground_truth = parse_ground_truth_file(os.path.join(data_dir, '..',\n    'ground_truth_file.txt'))\nlsi = _LSIWrapper(cache_dir=cache_dir, parent_id=uuid, mode='w')\nlsi_res, exp_var = lsi.fit_transform(n_components=100)\nassert lsi._load_pars() is not None\nlsi._load_model()\nif method == 'Categorization':\n    if use_lsi:\n        parent_id = lsi.mid\n        method = 'NearestNeighbor'\n    else:\n        parent_id = uuid\n        method = 'LogisticRegression'\n    cat = _CategorizerWrapper(cache_dir=cache_dir, parent_id=parent_id,\n        cv_n_folds=2)\n    cat.fe.db_.filenames_ = cat.fe.filenames_\n    index = cat.fe.db_._search_filenames(ground_truth.file_path.values)\n    try:\n        coefs, Y_train = cat.fit(index, ground_truth.is_relevant.values,\n            method=method)\n    except OptionalDependencyMissing:\n        raise SkipTest\n    Y_pred, md = cat.predict()\n    X_pred = np.arange(cat.fe.n_samples_, dtype='int')\n    idx_gt = cat.fe.db_._search_filenames(ground_truth.file_path.values)\n    scores = categorization_score(idx_gt, ground_truth.is_relevant.values,\n        X_pred, np.argmax(Y_pred, axis=1))\n    assert_allclose(scores['precision'], 1, rtol=0.5)\n    assert_allclose(scores['recall'], 1, rtol=0.7)\n    cat.delete()\nelif method == 'DuplicateDetection':\n    dd = _DuplicateDetectionWrapper(cache_dir=cache_dir, parent_id=uuid)\n    try:\n        dd.fit()\n    except ImportError:\n        raise SkipTest\n    cluster_id = dd.query(distance=10)\nelif method == 'Clustering':\n    if not use_hashing:\n        if use_lsi:\n            parent_id = lsi.mid\n            method = 'birch'\n        else:\n            parent_id = uuid\n            method = 'k_means'\n        cat = _ClusteringWrapper(cache_dir=cache_dir, parent_id=parent_id)\n        cm = getattr(cat, method)\n        labels = cm(2)\n        htree = cat._get_htree(cat.pipeline.data)\n        terms = cat.compute_labels(n_top_words=10)\n    else:\n        with pytest.raises(NotImplementedError):\n            _ClusteringWrapper(cache_dir=cache_dir, parent_id=uuid)\nelse:\n    raise ValueError\npytest.mark.parametrize('use_hashing, use_lsi, method', itertools.product([\n    False, True], [False, True], ['Categorization', 'DuplicateDetection',\n    'Clustering']))"
}