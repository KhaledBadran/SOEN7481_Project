{
    "functionName": "rmsprop",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"RMSProp updates\n\n    Scale learning rates by dividing with the moving average of the root mean\n    squared (RMS) gradients. See [1]_ for further description.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    rho : float or symbolic scalar\n        Gradient moving average decay factor\n    epsilon : float or symbolic scalar\n        Small value added for numerical stability\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    `rho` should be between 0 and 1. A value of `rho` close to 1 will decay the\n    moving average slowly and a value close to 0 will decay the moving average\n    fast.\n\n    Using the step size :math:`\\\\eta` and a decay factor :math:`\\\\rho` the\n    learning rate :math:`\\\\eta_t` is calculated as:\n\n    .. math::\n       r_t &= \\\\rho r_{t-1} + (1-\\\\rho)*g^2\\\\\\\\\n       \\\\eta_t &= \\\\frac{\\\\eta}{\\\\sqrt{r_t + \\\\epsilon}}\n\n    References\n    ----------\n    .. [1] Tieleman, T. and Hinton, G. (2012):\n           Neural Networks for Machine Learning, Lecture 6.5 - rmsprop.\n           Coursera. http://www.youtube.com/watch?v=O3sxAc4hxZU (formula @5:20)\n    \"\"\"\ngrads = get_or_compute_grads(loss_or_grads, params)\nupdates = OrderedDict()\none = T.constant(1)\nfor param, grad in zip(params, grads):\n    value = param.get_value(borrow=True)\n    accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    accu_new = rho * accu + (one - rho) * grad ** 2\n    updates[accu] = accu_new\n    updates[param] = param - learning_rate * grad / T.sqrt(accu_new + epsilon)\nreturn updates\n"
}