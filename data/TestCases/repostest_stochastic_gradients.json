{
    "functionName": "test_stochastic_gradients",
    "className": null,
    "fileName": "/GPflow_&_GPflow/tests_&_gpflow_&_models_&_test_svgp.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "\"\"\"\n    In response to bug #281, we need to make sure stochastic update\n    happens correctly in tf optimizer mode.\n    To do this compare stochastic updates with deterministic updates\n    that should be equivalent.\n\n    Data term in svgp likelihood is\n    \\\\sum_{i=1^N}E_{q(i)}[\\\\log p(y_i | f_i )\n\n    This sum is then approximated with an unbiased minibatch estimate.\n    In this test we substitute a deterministic analogue of the batchs\n    sampler for which we can predict the effects of different updates.\n    \"\"\"\nX, Y = np.atleast_2d(np.array([0.0, 1.0])).T, np.atleast_2d(np.array([-1.0,\n    3.0])).T\nZ = np.atleast_2d(np.array([0.5]))\ndef get_model(num_data):\n    return gpflow.models.SVGP(kernel=gpflow.kernels.SquaredExponential(),\n        num_data=num_data, likelihood=gpflow.likelihoods.Gaussian(),\n        inducing_variable=Z)\ndef training_loop(indices, num_data, max_iter):\n    model = get_model(num_data)\n    opt = tf.optimizers.SGD(learning_rate=0.001)\n    data = X[indices], Y[indices]\n    for _ in range(max_iter):\n        with tf.GradientTape() as tape:\n            loss = model.training_loss(data)\n        grads = tape.gradient(loss, model.trainable_variables)\n        opt.apply_gradients(zip(grads, model.trainable_variables))\n    return model\nmodel_1 = training_loop(indices_1, num_data=num_data1, max_iter=max_iter)\nmodel_2 = training_loop(indices_2, num_data=num_data2, max_iter=max_iter)\nassert _check_models_close(model_1, model_2)\npytest.mark.parametrize('indices_1, indices_2, num_data1, num_data2, max_iter',\n    [[[0, 1], [1, 0], 2, 2, 3], [[0, 1], [0, 0], 1, 2, 1], [[0, 0], [0, 1],\n    1, 1, 2]])"
}