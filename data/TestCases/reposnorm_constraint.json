{
    "functionName": "norm_constraint",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Max weight norm constraints and gradient clipping\n\n    This takes a TensorVariable and rescales it so that incoming weight\n    norms are below a specified constraint value. Vectors violating the\n    constraint are rescaled so that they are within the allowed range.\n\n    Parameters\n    ----------\n    tensor_var : TensorVariable\n        Theano expression for update, gradient, or other quantity.\n    max_norm : scalar\n        This value sets the maximum allowed value of any norm in\n        `tensor_var`.\n    norm_axes : sequence (list or tuple)\n        The axes over which to compute the norm.  This overrides the\n        default norm axes defined for the number of dimensions\n        in `tensor_var`. When this is not specified and `tensor_var` is a\n        matrix (2D), this is set to `(0,)`. If `tensor_var` is a 3D, 4D or\n        5D tensor, it is set to a tuple listing all axes but axis 0. The\n        former default is useful for working with dense layers, the latter\n        is useful for 1D, 2D and 3D convolutional layers.\n        (Optional)\n    epsilon : scalar, optional\n        Value used to prevent numerical instability when dividing by\n        very small or zero norms.\n\n    Returns\n    -------\n    TensorVariable\n        Input `tensor_var` with rescaling applied to weight vectors\n        that violate the specified constraints.\n\n    Examples\n    --------\n    >>> param = theano.shared(\n    ...     np.random.randn(100, 200).astype(theano.config.floatX))\n    >>> update = param + 100\n    >>> update = norm_constraint(update, 10)\n    >>> func = theano.function([], [], updates=[(param, update)])\n    >>> # Apply constrained update\n    >>> _ = func()\n    >>> from lasagne.utils import compute_norms\n    >>> norms = compute_norms(param.get_value())\n    >>> np.isclose(np.max(norms), 10)\n    True\n\n    Notes\n    -----\n    When `norm_axes` is not specified, the axes over which the norm is\n    computed depend on the dimensionality of the input variable. If it is\n    2D, it is assumed to come from a dense layer, and the norm is computed\n    over axis 0. If it is 3D, 4D or 5D, it is assumed to come from a\n    convolutional layer and the norm is computed over all trailing axes\n    beyond axis 0. For other uses, you should explicitly specify the axes\n    over which to compute the norm using `norm_axes`.\n    \"\"\"\nndim = tensor_var.ndim\nif norm_axes is not None:\n    sum_over = tuple(norm_axes)\nelif ndim == 2:\n    sum_over = 0,\nelif ndim in [3, 4, 5]:\n    sum_over = tuple(range(1, ndim))\nelse:\n    raise ValueError(\n        'Unsupported tensor dimensionality {}.Must specify `norm_axes`'.\n        format(ndim))\ndtype = np.dtype(theano.config.floatX).type\nnorms = T.sqrt(T.sum(T.sqr(tensor_var), axis=sum_over, keepdims=True))\ntarget_norms = T.clip(norms, 0, dtype(max_norm))\nconstrained_output = tensor_var * (target_norms / (dtype(epsilon) + norms))\nreturn constrained_output\n"
}