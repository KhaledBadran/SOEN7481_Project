{
    "functionName": "evaluate_mrz",
    "className": null,
    "fileName": "/konstantint_&_PassportEye/passporteye_&_mrz_&_scripts.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"\n    A script for evaluating the current MRZ recognition pipeline by applying it to a list of files in a directory and reporting how well it went.\n    \"\"\"\nparser = argparse.ArgumentParser(description=\n    'Run the MRZ OCR recognition algorithm on the sample test data, reporting the quality summary.'\n    )\nparser.add_argument('-j', '--jobs', default=1, type=int, help=\n    'Number of parallel jobs to run')\nparser.add_argument('-dd', '--data-dir', default=pkg_resources.\n    resource_filename('passporteye.mrz', 'testdata'), help=\n    'Read files from this directory instead of the package test files')\nparser.add_argument('-sd', '--success-dir', default=None, help=\n    'Copy files with successful (nonzero score) extraction results to this directory'\n    )\nparser.add_argument('-fd', '--fail-dir', default=None, help=\n    'Copy files with unsuccessful (zero score) extraction resutls to this directory'\n    )\nparser.add_argument('-rd', '--roi-dir', default=None, help=\n    'Extract ROIs to this directory')\nparser.add_argument('-l', '--limit', default=-1, type=int, help=\n    'Only process the first <limit> files in the directory.')\nparser.add_argument('--legacy', action='store_true', help=\n    'Use the \"legacy\" Tesseract OCR engine (--oem 0). Despite the name, it most often results in better results. It is not the default option, because it will only work if your Tesseract installation includes the legacy *.traineddata files. You can download them at https://github.com/tesseract-ocr/tesseract/wiki/Data-Files#data-files-for-version-400-november-29-2016'\n    )\nargs = parser.parse_args()\nfiles = sorted(glob.glob(os.path.join(args.data_dir, '*.*')))\nif args.limit >= 0:\n    files = files[0:args.limit]\nlogging.basicConfig(level=logging.INFO)\nlog = logging.getLogger('evaluate_mrz')\ntic = time.time()\npool = multiprocessing.Pool(args.jobs)\nlog.info('Preparing computation for %d files from %s', len(files), args.\n    data_dir)\nlog.info('Running %d workers', args.jobs)\nresults = []\nsave_roi = args.roi_dir is not None\nfor d in [args.success_dir, args.fail_dir, args.roi_dir]:\n    if d is not None and not os.path.isdir(d):\n        os.mkdir(d)\ndef valid_score(mrz_):\n    return 0 if mrz_ is None else mrz_.valid_score\ndef score_change_type(filename, mrz_):\n    try:\n        new_score = valid_score(mrz_)\n        old_score = int(os.path.basename(filename).split('_')[0])\n        schange = new_score - old_score\n        return '=' if schange == 0 else '>' if schange > 0 else '<'\n    except Exception:\n        return '?'\nmethod_stats = Counter()\nextra_params = '--oem 0' if args.legacy else ''\nfor result in pool.imap_unordered(process_file, [(f, save_roi, extra_params\n    ) for f in files]):\n    filename, mrz_, walltime = result\n    results.append(result)\n    log.info('Processed %s in %0.2fs (score %d) [%s]', os.path.basename(\n        filename), walltime, valid_score(mrz_), score_change_type(filename,\n        mrz_))\n    log.debug('\\t%s', mrz_)\n    vs = valid_score(mrz_)\n    if args.success_dir is not None and vs > 0:\n        shutil.copyfile(filename, os.path.join(args.success_dir, '%d_%s' %\n            (vs, os.path.basename(filename))))\n    if args.fail_dir is not None and vs == 0:\n        shutil.copyfile(filename, os.path.join(args.fail_dir, '%d_%s' % (vs,\n            os.path.basename(filename))))\n    if args.roi_dir is not None and mrz_ is not None and 'roi' in mrz_.aux:\n        roi_fn = '%d_roi_%s.png' % (vs, os.path.basename(filename))\n        io.imsave(os.path.join(args.roi_dir, roi_fn), mrz_.aux['roi'])\n    if vs > 0 and 'method' in mrz_.aux:\n        method_stats[mrz_.aux['method']] += 1\nnum_files = len(results)\nscore_changes = [score_change_type(fn, mrz_) for fn, mrz_, wt in results]\nscores = [valid_score(mrz_) for fn, mrz_, wt in results]\nnum_perfect = scores.count(100)\nnum_invalid = scores.count(0)\ntotal_score = sum(scores)\ntotal_computation_walltime = sum([wt for fn, mrz_, wt in results])\ntotal_walltime = time.time() - tic\nlog.info('Completed')\nprint('Walltime:          %0.2fs' % total_walltime)\nprint('Compute walltime:  %0.2fs' % total_computation_walltime)\nprint('Processed files:   %d' % num_files)\nprint('Perfect parses:    %d' % num_perfect)\nprint('Invalid parses:    %d' % num_invalid)\nprint('Improved parses:   %d' % len([x for x in score_changes if x == '>']))\nprint('Worsened parses:   %d' % len([x for x in score_changes if x == '<']))\nprint('Total score:       %d' % total_score)\nprint('Mean score:        %0.2f' % (float(total_score) / num_files))\nprint('Mean compute time: %0.2fs' % (total_computation_walltime / num_files))\nprint('Methods used:')\nfor stat in method_stats.most_common():\n    print('  %s: %d' % stat)\n"
}