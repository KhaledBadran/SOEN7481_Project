{
    "functionName": "nesterov_momentum",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Stochastic Gradient Descent (SGD) updates with Nesterov momentum\n\n    Generates update expressions of the form:\n\n    * ``velocity := momentum * velocity - learning_rate * gradient``\n    * ``param := param + momentum * velocity - learning_rate * gradient``\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    momentum : float or symbolic scalar, optional\n        The amount of momentum to apply. Higher momentum results in\n        smoothing over more update steps. Defaults to 0.9.\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    Higher momentum also results in larger update steps. To counter that,\n    you can optionally scale your learning rate by `1 - momentum`.\n\n    The classic formulation of Nesterov momentum (or Nesterov accelerated\n    gradient) requires the gradient to be evaluated at the predicted next\n    position in parameter space. Here, we use the formulation described at\n    https://github.com/lisa-lab/pylearn2/pull/136#issuecomment-10381617,\n    which allows the gradient to be evaluated at the current parameters.\n\n    See Also\n    --------\n    apply_nesterov_momentum : Function applying momentum to updates\n    \"\"\"\nupdates = sgd(loss_or_grads, params, learning_rate)\nreturn apply_nesterov_momentum(updates, momentum=momentum)\n"
}