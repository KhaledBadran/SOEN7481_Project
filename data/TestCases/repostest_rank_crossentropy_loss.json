{
    "functionName": "test_rank_crossentropy_loss",
    "className": null,
    "fileName": "/NTMC-Community_&_MatchZoo-py/tests_&_test_losses.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "losses.neg_num = 1\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\ntrue_value = torch.Tensor([[1.0], [0.0], [0.0], [1.0]])\npred_value = torch.Tensor([[0.8], [0.1], [0.8], [0.1]])\nexpected_loss = torch.Tensor([(-np.log(softmax([0.8, 0.1])[0]) - np.log(\n    softmax([0.8, 0.1])[1])) / 2])\nloss = losses.RankCrossEntropyLoss()(pred_value, true_value)\nassert torch.isclose(expected_loss, loss)\ntrue_value = torch.Tensor([[1.0], [0.0], [0.0], [0.0], [1.0], [0.0]])\npred_value = torch.Tensor([[0.8], [0.1], [0.1], [0.8], [0.1], [0.1]])\nexpected_loss = torch.Tensor([(-np.log(softmax([0.8, 0.1, 0.1])[0]) - np.\n    log(softmax([0.8, 0.1, 0.1])[1])) / 2])\nloss = losses.RankCrossEntropyLoss(num_neg=2)(pred_value, true_value)\nassert torch.isclose(expected_loss, loss)\n"
}