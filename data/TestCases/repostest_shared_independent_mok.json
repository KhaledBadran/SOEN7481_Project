{
    "functionName": "test_shared_independent_mok",
    "className": null,
    "fileName": "/GPflow_&_GPflow/tests_&_gpflow_&_conditionals_&_test_multioutput.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "\"\"\"\n    In this test we use the same kernel and the same inducing inducing\n    for each of the outputs. The outputs are considered to be uncorrelated.\n    This is how GPflow handled multiple outputs before the multioutput framework was added.\n    We compare three models here:\n        1) an ineffient one, where we use a SharedIndepedentMok with InducingPoints.\n           This combination will uses a Kff of size N x P x N x P, Kfu if size N x P x M x P\n           which is extremely inefficient as most of the elements are zero.\n        2) efficient: SharedIndependentMok and SharedIndependentMof\n           This combinations uses the most efficient form of matrices\n        3) the old way, efficient way: using Kernel and InducingPoints\n        Model 2) and 3) follow more or less the same code path.\n    \"\"\"\nnp.random.seed(0)\nq_mu_1 = np.random.randn(Data.M * Data.P, 1)\nq_sqrt_1 = np.tril(np.random.randn(Data.M * Data.P, Data.M * Data.P))[None, ...\n    ]\nkernel_1 = mk.SharedIndependent(SquaredExponential(variance=0.5,\n    lengthscales=1.2), Data.P)\ninducing_variable = InducingPoints(Data.X[:Data.M, (...)])\nmodel_1 = SVGP(kernel_1, Gaussian(), inducing_variable, q_mu=q_mu_1, q_sqrt\n    =q_sqrt_1, num_latent_gps=Data.Y.shape[-1])\nset_trainable(model_1, False)\nset_trainable(model_1.q_sqrt, True)\ngpflow.optimizers.Scipy().minimize(model_1.training_loss_closure(Data.data),\n    variables=model_1.trainable_variables, options=dict(maxiter=500),\n    method='BFGS', compile=True)\nq_mu_2 = np.reshape(q_mu_1, [Data.M, Data.P])\nq_sqrt_2 = np.array([np.tril(np.random.randn(Data.M, Data.M)) for _ in\n    range(Data.P)])\nkernel_2 = SquaredExponential(variance=0.5, lengthscales=1.2)\ninducing_variable_2 = InducingPoints(Data.X[:Data.M, (...)])\nmodel_2 = SVGP(kernel_2, Gaussian(), inducing_variable_2, num_latent_gps=\n    Data.P, q_mu=q_mu_2, q_sqrt=q_sqrt_2)\nset_trainable(model_2, False)\nset_trainable(model_2.q_sqrt, True)\ngpflow.optimizers.Scipy().minimize(model_2.training_loss_closure(Data.data),\n    variables=model_2.trainable_variables, options=dict(maxiter=500),\n    method='BFGS', compile=True)\nq_mu_3 = np.reshape(q_mu_1, [Data.M, Data.P])\nq_sqrt_3 = np.array([np.tril(np.random.randn(Data.M, Data.M)) for _ in\n    range(Data.P)])\nkernel_3 = mk.SharedIndependent(SquaredExponential(variance=0.5,\n    lengthscales=1.2), Data.P)\ninducing_variable_3 = mf.SharedIndependentInducingVariables(InducingPoints(\n    Data.X[:Data.M, (...)]))\nmodel_3 = SVGP(kernel_3, Gaussian(), inducing_variable_3, num_latent_gps=\n    Data.P, q_mu=q_mu_3, q_sqrt=q_sqrt_3)\nset_trainable(model_3, False)\nset_trainable(model_3.q_sqrt, True)\ngpflow.optimizers.Scipy().minimize(model_3.training_loss_closure(Data.data),\n    variables=model_3.trainable_variables, options=dict(maxiter=500),\n    method='BFGS', compile=True)\ncheck_equality_predictions(Data.data, [model_1, model_2, model_3])\n"
}