{
    "functionName": "apply_nesterov_momentum",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Returns a modified update dictionary including Nesterov momentum\n\n    Generates update expressions of the form:\n\n    * ``velocity := momentum * velocity + updates[param] - param``\n    * ``param := param + momentum * velocity + updates[param] - param``\n\n    Parameters\n    ----------\n    updates : OrderedDict\n        A dictionary mapping parameters to update expressions\n    params : iterable of shared variables, optional\n        The variables to apply momentum to. If omitted, will apply\n        momentum to all `updates.keys()`.\n    momentum : float or symbolic scalar, optional\n        The amount of momentum to apply. Higher momentum results in\n        smoothing over more update steps. Defaults to 0.9.\n\n    Returns\n    -------\n    OrderedDict\n        A copy of `updates` with momentum updates for all `params`.\n\n    Notes\n    -----\n    Higher momentum also results in larger update steps. To counter that,\n    you can optionally scale your learning rate by `1 - momentum`.\n\n    The classic formulation of Nesterov momentum (or Nesterov accelerated\n    gradient) requires the gradient to be evaluated at the predicted next\n    position in parameter space. Here, we use the formulation described at\n    https://github.com/lisa-lab/pylearn2/pull/136#issuecomment-10381617,\n    which allows the gradient to be evaluated at the current parameters.\n\n    See Also\n    --------\n    nesterov_momentum : Shortcut applying Nesterov momentum to SGD updates\n    \"\"\"\nif params is None:\n    params = updates.keys()\nupdates = OrderedDict(updates)\nfor param in params:\n    value = param.get_value(borrow=True)\n    velocity = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    x = momentum * velocity + updates[param] - param\n    updates[velocity] = x\n    updates[param] = momentum * x + updates[param]\nreturn updates\n"
}