{
    "functionName": "backprop_based_models",
    "className": null,
    "fileName": "/viebboy_&_PyGOP/test_&_models_&_test_all_models.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "model_path = os.path.join(tmp_dir, 'test_model')\nif os.path.exists(model_path):\n    shutil.rmtree(model_path)\nos.mkdir(model_path)\ntrain_seed = np.random.randint(0, 1000)\nval_seed = np.random.randint(0, 1000)\ntest_seed = np.random.randint(0, 1000)\ntrain_func = utility.get_generator\ntrain_data = [INPUT_DIM, OUTPUT_DIM, BATCH_SIZE, STEPS, train_seed]\nval_func = utility.get_generator\nval_data = [INPUT_DIM, OUTPUT_DIM, BATCH_SIZE, STEPS, val_seed]\ntest_func = utility.get_generator\ntest_data = [INPUT_DIM, OUTPUT_DIM, BATCH_SIZE, STEPS, test_seed]\nmodel = MODELS[NAMES.index(model_name)]()\nparams = model.get_default_parameters()\nparams['tmp_dir'] = tmp_dir\nparams['model_name'] = model_name\nparams['input_dim'] = INPUT_DIM\nparams['output_dim'] = OUTPUT_DIM\nparams['loss'] = utility.mean_absolute_error_keras\nparams['metrics'] = ['mse', utility.mean_absolute_error_keras]\nparams['special_metrics'] = [utility.mean_absolute_error_numpy]\nparams['convergence_measure'] = 'mean_absolute_error_keras'\nparams['direction'] = 'lower'\nparams['search_computation'] = 'cpu', 2\nparams['finetune_computation'] = 'cpu', 1\nparams['max_topology'] = [4]\nparams['nodal_set'] = utility.get_nodal_set()\nparams['pool_set'] = utility.get_pool_set()\nparams['activation_set'] = utility.get_activation_set()\nparams['lr_train'] = 0.001, 0.0001\nparams['epoch_train'] = 1, 1\nparams['lr_finetune'] = 0.001, 0.0001\nparams['epoch_finetune'] = 1, 1\nparams['memory_type'] = mem_type\nparams['memory_regularizer'] = 0.1\nparams['optimizer'] = random.choice(['sgd', 'rmsprop', 'adagrad',\n    'adadelta', 'adam', 'adamax', 'nadam'])\nparams['optimizer_parameters'] = random.choice([None, {'lr': 0.01}])\nmodel.fit(params, train_func, train_data, val_func, val_data, test_func,\n    test_data)\ntest_func = utility.get_test_generator\ntest_data_eval = [np.random.rand(BATCH_SIZE * STEPS, INPUT_DIM), np.random.\n    rand(BATCH_SIZE * STEPS, OUTPUT_DIM), BATCH_SIZE]\ntest_data_pred = [np.random.rand(BATCH_SIZE * STEPS, INPUT_DIM), None,\n    BATCH_SIZE]\ntest_performance_bef = model.evaluate(test_func, test_data_eval, params[\n    'metrics'], params['special_metrics'], params['finetune_computation'])\ntest_pred_bef = model.predict(test_func, test_data_pred, params[\n    'finetune_computation'])\nmodel.save(os.path.join(model_path, model_name + '_pretrained.pickle'))\nmodel = MODELS[NAMES.index(model_name)]()\nmodel.load(os.path.join(model_path, model_name + '_pretrained.pickle'))\ntest_performance_af = model.evaluate(test_func, test_data_eval, params[\n    'metrics'], params['special_metrics'], params['finetune_computation'])\ntest_pred_af = model.predict(test_func, test_data_pred, params[\n    'finetune_computation'])\nassert np.allclose(test_pred_bef, test_pred_af)\nfor metric in test_performance_bef.keys():\n    assert np.allclose(test_performance_bef[metric], test_performance_af[\n        metric])\nshutil.rmtree(model_path)\n"
}