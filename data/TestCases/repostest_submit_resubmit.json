{
    "functionName": "test_submit_resubmit",
    "className": null,
    "fileName": "/bio-phys_&_MDBenchmark/mdbenchmark_&_tests_&_test_submit.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "\"\"\"Test that we cannot submit a benchmark system that was already submitted,\n       unless we force it.\n    \"\"\"\nwith tmpdir.as_cwd():\n    result = cli_runner.invoke(cli, ['submit', '--directory=look_here/'],\n        '--yes')\n    assert result.exit_code == 1\n    assert result.output == 'ERROR No benchmarks found.\\n'\n    result = cli_runner.invoke(cli, ['submit', '--directory={}'.format(data\n        ['analyze-files-gromacs']), '--yes'])\n    df = pd.read_csv(data['analyze-files-gromacs-consolidated.csv'],\n        index_col=0)\n    s = PrintDataFrame(df, False)\n    output = \"\"\"ERROR All generated benchmarks were already started once. You can force a restart with --force.\n\"\"\"\n    assert result.exit_code == 1\n    assert result.output == output\n    monkeypatch.setattr('subprocess.call', lambda x: True)\n    monkeypatch.setattr('mdbenchmark.cli.submit.get_batch_command', lambda :\n        'sbatch')\n    monkeypatch.setattr('mdbenchmark.cli.submit.detect_md_engine', lambda x:\n        gromacs)\n    monkeypatch.setattr('mdbenchmark.submit.cleanup_before_restart', lambda\n        engine, sim: True)\n    output = 'Benchmark Summary:\\n' + s + '\\nThe above benchmarks will be submitted.\\n' + \"\"\"Submitting a total of 5 benchmarks.\n\"\"\" + \"\"\"Submitted all benchmarks. Run mdbenchmark analyze once they are finished to get the results.\n\"\"\"\n    result = cli_runner.invoke(cli, ['submit', '--directory={}'.format(data\n        ['analyze-files-gromacs']), '--force', '--yes'])\n    assert result.exit_code == 0\n    assert result.output == output\npytest.mark.skip(reason='monkeypatching is a problem. skip for now.')"
}