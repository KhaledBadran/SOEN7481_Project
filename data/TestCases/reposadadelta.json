{
    "functionName": "adadelta",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\" Adadelta updates\n\n    Scale learning rates by the ratio of accumulated gradients to accumulated\n    updates, see [1]_ and notes for further description.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    rho : float or symbolic scalar\n        Squared gradient moving average decay factor\n    epsilon : float or symbolic scalar\n        Small value added for numerical stability\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    rho should be between 0 and 1. A value of rho close to 1 will decay the\n    moving average slowly and a value close to 0 will decay the moving average\n    fast.\n\n    rho = 0.95 and epsilon=1e-6 are suggested in the paper and reported to\n    work for multiple datasets (MNIST, speech).\n\n    In the paper, no learning rate is considered (so learning_rate=1.0).\n    Probably best to keep it at this value.\n    epsilon is important for the very first update (so the numerator does\n    not become 0).\n\n    Using the step size eta and a decay factor rho the learning rate is\n    calculated as:\n\n    .. math::\n       r_t &= \\\\rho r_{t-1} + (1-\\\\rho)*g^2\\\\\\\\\n       \\\\eta_t &= \\\\eta \\\\frac{\\\\sqrt{s_{t-1} + \\\\epsilon}}\n                             {\\\\sqrt{r_t + \\\\epsilon}}\\\\\\\\\n       s_t &= \\\\rho s_{t-1} + (1-\\\\rho)*(\\\\eta_t*g)^2\n\n    References\n    ----------\n    .. [1] Zeiler, M. D. (2012):\n           ADADELTA: An Adaptive Learning Rate Method.\n           arXiv Preprint arXiv:1212.5701.\n    \"\"\"\ngrads = get_or_compute_grads(loss_or_grads, params)\nupdates = OrderedDict()\none = T.constant(1)\nfor param, grad in zip(params, grads):\n    value = param.get_value(borrow=True)\n    accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    delta_accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    accu_new = rho * accu + (one - rho) * grad ** 2\n    updates[accu] = accu_new\n    update = grad * T.sqrt(delta_accu + epsilon) / T.sqrt(accu_new + epsilon)\n    updates[param] = param - learning_rate * update\n    delta_accu_new = rho * delta_accu + (one - rho) * update ** 2\n    updates[delta_accu] = delta_accu_new\nreturn updates\n"
}