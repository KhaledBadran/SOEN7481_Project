{
    "functionName": "test_wordpiece_tokenizer",
    "className": "TokenizationTest",
    "fileName": "/kpi6research_&_Bert-as-a-Library/BertLibrary_&_bert_&_tokenization_test.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "vocab_tokens = ['[UNK]', '[CLS]', '[SEP]', 'want', '##want', '##ed', 'wa',\n    'un', 'runn', '##ing']\nvocab = {}\nfor i, token in enumerate(vocab_tokens):\n    vocab[token] = i\ntokenizer = tokenization.WordpieceTokenizer(vocab=vocab)\nself.assertAllEqual(tokenizer.tokenize(''), [])\nself.assertAllEqual(tokenizer.tokenize('unwanted running'), ['un', '##want',\n    '##ed', 'runn', '##ing'])\nself.assertAllEqual(tokenizer.tokenize('unwantedX running'), ['[UNK]',\n    'runn', '##ing'])\n"
}