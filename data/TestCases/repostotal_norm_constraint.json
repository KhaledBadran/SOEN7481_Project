{
    "functionName": "total_norm_constraint",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Rescales a list of tensors based on their combined norm\n\n    If the combined norm of the input tensors exceeds the threshold then all\n    tensors are rescaled such that the combined norm is equal to the threshold.\n\n    Scaling the norms of the gradients is often used when training recurrent\n    neural networks [1]_.\n\n    Parameters\n    ----------\n    tensor_vars : List of TensorVariables.\n        Tensors to be rescaled.\n    max_norm : float\n        Threshold value for total norm.\n    epsilon : scalar, optional\n        Value used to prevent numerical instability when dividing by\n        very small or zero norms.\n    return_norm : bool\n        If true the total norm is also returned.\n\n    Returns\n    -------\n    tensor_vars_scaled : list of TensorVariables\n        The scaled tensor variables.\n    norm : Theano scalar\n        The combined norms of the input variables prior to rescaling,\n        only returned if ``return_norms=True``.\n\n    Examples\n    --------\n    >>> from lasagne.layers import InputLayer, DenseLayer\n    >>> import lasagne\n    >>> from lasagne.updates import sgd, total_norm_constraint\n    >>> x = T.matrix()\n    >>> y = T.ivector()\n    >>> l_in = InputLayer((5, 10))\n    >>> l1 = DenseLayer(l_in, num_units=7, nonlinearity=T.nnet.softmax)\n    >>> output = lasagne.layers.get_output(l1, x)\n    >>> cost = T.mean(T.nnet.categorical_crossentropy(output, y))\n    >>> all_params = lasagne.layers.get_all_params(l1)\n    >>> all_grads = T.grad(cost, all_params)\n    >>> scaled_grads = total_norm_constraint(all_grads, 5)\n    >>> updates = sgd(scaled_grads, all_params, learning_rate=0.1)\n\n    Notes\n    -----\n    The total norm can be used to monitor training.\n\n    References\n    ----------\n    .. [1] Sutskever, I., Vinyals, O., & Le, Q. V. (2014): Sequence to sequence\n       learning with neural networks. In Advances in Neural Information\n       Processing Systems (pp. 3104-3112).\n    \"\"\"\nnorm = T.sqrt(sum(T.sum(tensor ** 2) for tensor in tensor_vars))\ndtype = np.dtype(theano.config.floatX).type\ntarget_norm = T.clip(norm, 0, dtype(max_norm))\nmultiplier = target_norm / (dtype(epsilon) + norm)\ntensor_vars_scaled = [(step * multiplier) for step in tensor_vars]\nif return_norm:\n    return tensor_vars_scaled, norm\nelse:\n    return tensor_vars_scaled\n"
}