{
    "functionName": "aggregate",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_objectives.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Aggregates an element- or item-wise loss to a scalar loss.\n\n    Parameters\n    ----------\n    loss : Theano tensor\n        The loss expression to aggregate.\n    weights : Theano tensor, optional\n        The weights for each element or item, must be broadcastable to\n        the same shape as `loss` if given. If omitted, all elements will\n        be weighted the same.\n    mode : {'mean', 'sum', 'normalized_sum'}\n        Whether to aggregate by averaging, by summing or by summing and\n        dividing by the total weights (which requires `weights` to be given).\n\n    Returns\n    -------\n    Theano scalar\n        A scalar loss expression suitable for differentiation.\n\n    Notes\n    -----\n    By supplying binary weights (i.e., only using values 0 and 1), this\n    function can also be used for masking out particular entries in the\n    loss expression. Note that masked entries still need to be valid\n    values, not-a-numbers (NaNs) will propagate through.\n\n    When applied to batch-wise loss expressions, setting `mode` to\n    ``'normalized_sum'`` ensures that the loss per batch is of a similar\n    magnitude, independent of associated weights. However, it means that\n    a given data point contributes more to the loss when it shares a batch\n    with low-weighted or masked data points than with high-weighted ones.\n    \"\"\"\nif weights is not None:\n    loss = loss * weights\nif mode == 'mean':\n    return loss.mean()\nelif mode == 'sum':\n    return loss.sum()\nelif mode == 'normalized_sum':\n    if weights is None:\n        raise ValueError(\"require weights for mode='normalized_sum'\")\n    return loss.sum() / weights.sum()\nelse:\n    raise ValueError(\n        \"mode must be 'mean', 'sum' or 'normalized_sum', got %r\" % mode)\n"
}