{
    "functionName": "test_update_returntype",
    "className": "TestUpdateFunctions",
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_tests_&_test_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": true,
    "Body": "\"\"\"Checks whether lasagne.updates handles float32 inputs correctly\"\"\"\nfloatX_ = theano.config.floatX\ntheano.config.floatX = 'float32'\ntry:\n    A = theano.shared(lasagne.utils.floatX([1, 1, 1]))\n    B = theano.shared(lasagne.utils.floatX([1, 1, 1]))\n    update_func = getattr(lasagne.updates, method)\n    updates = update_func(self.f(A) + self.f(B), [A, B], **kwargs)\n    assert all(v.dtype == 'float32' for v in updates)\n    for param in kwargs:\n        kwargs[param] = np.float32(kwargs[param])\n    updates = update_func(self.f(A) + self.f(B), [A, B], **kwargs)\n    assert all(v.dtype == 'float32' for v in updates)\nfinally:\n    theano.config.floatX = floatX_\npytest.mark.parametrize('method, kwargs', [['sgd', {'learning_rate': 0.1}],\n    ['momentum', {'learning_rate': 0.1, 'momentum': 0.5}], [\n    'nesterov_momentum', {'learning_rate': 0.1, 'momentum': 0.5}], [\n    'adagrad', {'learning_rate': 0.1, 'epsilon': 1e-06}], ['rmsprop', {\n    'learning_rate': 0.01, 'rho': 0.9, 'epsilon': 1e-06}], ['adadelta', {\n    'learning_rate': 0.01, 'rho': 0.9, 'epsilon': 1e-06}], ['adam', {\n    'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08}],\n    ['adamax', {'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.999,\n    'epsilon': 1e-08}], ['amsgrad', {'learning_rate': 0.01, 'beta1': 0.9,\n    'beta2': 0.999, 'epsilon': 1e-08}]])"
}