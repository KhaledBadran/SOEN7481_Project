{
    "functionName": "binary_hinge_loss",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_objectives.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Computes the binary hinge loss between predictions and targets.\n\n    .. math:: L_i = \\\\max(0, \\\\delta - t_i p_i)\n\n    Parameters\n    ----------\n    predictions : Theano tensor\n        Predictions in (0, 1), such as sigmoidal output of a neural network\n        (or log-odds of predictions depending on `log_odds`).\n    targets : Theano tensor\n        Targets in {0, 1} (or in {-1, 1} depending on `binary`), such as\n        ground truth labels.\n    delta : scalar, default 1\n        The hinge loss margin\n    log_odds : bool, default None\n        ``False`` if predictions are sigmoid outputs in (0, 1), ``True`` if\n        predictions are sigmoid inputs, or log-odds. If ``None``, will assume\n        ``True``, but warn that the default will change to ``False``.\n    binary : bool, default True\n        ``True`` if targets are in {0, 1}, ``False`` if they are in {-1, 1}\n\n    Returns\n    -------\n    Theano tensor\n        An expression for the element-wise binary hinge loss\n\n    Notes\n    -----\n    This is an alternative to the binary cross-entropy loss for binary\n    classification problems.\n\n    Note that it is a drop-in replacement only when giving ``log_odds=False``.\n    Otherwise, it requires log-odds rather than sigmoid outputs. Be aware that\n    depending on the Theano version, ``log_odds=False`` with a sigmoid\n    output layer may be less stable than ``log_odds=True`` with a linear layer.\n    \"\"\"\nif log_odds is None:\n    raise FutureWarning(\n        'The `log_odds` argument to `binary_hinge_loss` will change its default to `False` in a future version. Explicitly give `log_odds=True` to retain current behavior in your code, but also check the documentation if this is what you want.'\n        )\n    log_odds = True\nif not log_odds:\n    predictions = theano.tensor.log(predictions / (1 - predictions))\nif binary:\n    targets = 2 * targets - 1\npredictions, targets = align_targets(predictions, targets)\nreturn theano.tensor.nnet.relu(delta - predictions * targets)\n"
}