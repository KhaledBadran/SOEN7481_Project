{
    "functionName": "momentum",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Stochastic Gradient Descent (SGD) updates with momentum\n\n    Generates update expressions of the form:\n\n    * ``velocity := momentum * velocity - learning_rate * gradient``\n    * ``param := param + velocity``\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    momentum : float or symbolic scalar, optional\n        The amount of momentum to apply. Higher momentum results in\n        smoothing over more update steps. Defaults to 0.9.\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    Higher momentum also results in larger update steps. To counter that,\n    you can optionally scale your learning rate by `1 - momentum`.\n\n    See Also\n    --------\n    apply_momentum : Generic function applying momentum to updates\n    nesterov_momentum : Nesterov's variant of SGD with momentum\n    \"\"\"\nupdates = sgd(loss_or_grads, params, learning_rate)\nreturn apply_momentum(updates, momentum=momentum)\n"
}