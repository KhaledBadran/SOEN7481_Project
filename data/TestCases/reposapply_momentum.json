{
    "functionName": "apply_momentum",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Returns a modified update dictionary including momentum\n\n    Generates update expressions of the form:\n\n    * ``velocity := momentum * velocity + updates[param] - param``\n    * ``param := param + velocity``\n\n    Parameters\n    ----------\n    updates : OrderedDict\n        A dictionary mapping parameters to update expressions\n    params : iterable of shared variables, optional\n        The variables to apply momentum to. If omitted, will apply\n        momentum to all `updates.keys()`.\n    momentum : float or symbolic scalar, optional\n        The amount of momentum to apply. Higher momentum results in\n        smoothing over more update steps. Defaults to 0.9.\n\n    Returns\n    -------\n    OrderedDict\n        A copy of `updates` with momentum updates for all `params`.\n\n    Notes\n    -----\n    Higher momentum also results in larger update steps. To counter that,\n    you can optionally scale your learning rate by `1 - momentum`.\n\n    See Also\n    --------\n    momentum : Shortcut applying momentum to SGD updates\n    \"\"\"\nif params is None:\n    params = updates.keys()\nupdates = OrderedDict(updates)\nfor param in params:\n    value = param.get_value(borrow=True)\n    velocity = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    x = momentum * velocity + updates[param]\n    updates[velocity] = x - param\n    updates[param] = x\nreturn updates\n"
}