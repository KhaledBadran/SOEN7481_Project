{
    "functionName": "adamax",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Adamax updates\n\n    Adamax updates implemented as in [1]_. This is a variant of of the Adam\n    algorithm based on the infinity norm.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        Learning rate\n    beta1 : float or symbolic scalar\n        Exponential decay rate for the first moment estimates.\n    beta2 : float or symbolic scalar\n        Exponential decay rate for the weighted infinity norm estimates.\n    epsilon : float or symbolic scalar\n        Constant for numerical stability.\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    References\n    ----------\n    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n           Adam: A Method for Stochastic Optimization.\n           arXiv preprint arXiv:1412.6980.\n    \"\"\"\nall_grads = get_or_compute_grads(loss_or_grads, params)\nt_prev = theano.shared(utils.floatX(0.0))\nupdates = OrderedDict()\none = T.constant(1)\nt = t_prev + 1\na_t = learning_rate / (one - beta1 ** t)\nfor param, g_t in zip(params, all_grads):\n    value = param.get_value(borrow=True)\n    m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    u_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    m_t = beta1 * m_prev + (one - beta1) * g_t\n    u_t = T.maximum(beta2 * u_prev, abs(g_t))\n    step = a_t * m_t / (u_t + epsilon)\n    updates[m_prev] = m_t\n    updates[u_prev] = u_t\n    updates[param] = param - step\nupdates[t_prev] = t\nreturn updates\n"
}