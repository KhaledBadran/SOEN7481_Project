{
    "functionName": "amsgrad",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"AMSGrad updates\n\n    AMSGrad updates implemented as in [1]_.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        Learning rate\n    beta1 : float or symbolic scalar\n        Exponential decay rate for the first moment estimates.\n    beta2 : float or symbolic scalar\n        Exponential decay rate for the second moment estimates.\n    epsilon : float or symbolic scalar\n        Constant for numerical stability.\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    References\n    ----------\n    .. [1] https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\nall_grads = get_or_compute_grads(loss_or_grads, params)\nt_prev = theano.shared(utils.floatX(0.0))\nupdates = OrderedDict()\none = T.constant(1)\nt = t_prev + 1\na_t = learning_rate * T.sqrt(one - beta2 ** t) / (one - beta1 ** t)\nfor param, g_t in zip(params, all_grads):\n    value = param.get_value(borrow=True)\n    m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    v_hat_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    m_t = beta1 * m_prev + (one - beta1) * g_t\n    v_t = beta2 * v_prev + (one - beta2) * g_t ** 2\n    v_hat_t = T.maximum(v_hat_prev, v_t)\n    step = a_t * m_t / (T.sqrt(v_hat_t) + epsilon)\n    updates[m_prev] = m_t\n    updates[v_prev] = v_t\n    updates[v_hat_prev] = v_hat_t\n    updates[param] = param - step\nupdates[t_prev] = t\nreturn updates\n"
}