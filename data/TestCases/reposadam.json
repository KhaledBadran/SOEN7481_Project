{
    "functionName": "adam",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Adam updates\n\n    Adam updates implemented as in [1]_.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        Learning rate\n    beta1 : float or symbolic scalar\n        Exponential decay rate for the first moment estimates.\n    beta2 : float or symbolic scalar\n        Exponential decay rate for the second moment estimates.\n    epsilon : float or symbolic scalar\n        Constant for numerical stability.\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    The paper [1]_ includes an additional hyperparameter lambda. This is only\n    needed to prove convergence of the algorithm and has no practical use\n    (personal communication with the authors), it is therefore omitted here.\n\n    References\n    ----------\n    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n           Adam: A Method for Stochastic Optimization.\n           arXiv preprint arXiv:1412.6980.\n    \"\"\"\nall_grads = get_or_compute_grads(loss_or_grads, params)\nt_prev = theano.shared(utils.floatX(0.0))\nupdates = OrderedDict()\none = T.constant(1)\nt = t_prev + 1\na_t = learning_rate * T.sqrt(one - beta2 ** t) / (one - beta1 ** t)\nfor param, g_t in zip(params, all_grads):\n    value = param.get_value(borrow=True)\n    m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    m_t = beta1 * m_prev + (one - beta1) * g_t\n    v_t = beta2 * v_prev + (one - beta2) * g_t ** 2\n    step = a_t * m_t / (T.sqrt(v_t) + epsilon)\n    updates[m_prev] = m_t\n    updates[v_prev] = v_t\n    updates[param] = param - step\nupdates[t_prev] = t\nreturn updates\n"
}