{
    "functionName": "adagrad",
    "className": null,
    "fileName": "/Lasagne_&_Lasagne/lasagne_&_updates.py",
    "projectName": "repos",
    "Label": false,
    "isTest": false,
    "Body": "\"\"\"Adagrad updates\n\n    Scale learning rates by dividing with the square root of accumulated\n    squared gradients. See [1]_ for further description.\n\n    Parameters\n    ----------\n    loss_or_grads : symbolic expression or list of expressions\n        A scalar loss expression, or a list of gradient expressions\n    params : list of shared variables\n        The variables to generate update expressions for\n    learning_rate : float or symbolic scalar\n        The learning rate controlling the size of update steps\n    epsilon : float or symbolic scalar\n        Small value added for numerical stability\n\n    Returns\n    -------\n    OrderedDict\n        A dictionary mapping each parameter to its update expression\n\n    Notes\n    -----\n    Using step size eta Adagrad calculates the learning rate for feature i at\n    time step t as:\n\n    .. math:: \\\\eta_{t,i} = \\\\frac{\\\\eta}\n       {\\\\sqrt{\\\\sum^t_{t^\\\\prime} g^2_{t^\\\\prime,i}+\\\\epsilon}} g_{t,i}\n\n    as such the learning rate is monotonically decreasing.\n\n    Epsilon is not included in the typical formula, see [2]_.\n\n    References\n    ----------\n    .. [1] Duchi, J., Hazan, E., & Singer, Y. (2011):\n           Adaptive subgradient methods for online learning and stochastic\n           optimization. JMLR, 12:2121-2159.\n\n    .. [2] Chris Dyer:\n           Notes on AdaGrad. http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf\n    \"\"\"\ngrads = get_or_compute_grads(loss_or_grads, params)\nupdates = OrderedDict()\nfor param, grad in zip(params, grads):\n    value = param.get_value(borrow=True)\n    accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n        broadcastable=param.broadcastable)\n    accu_new = accu + grad ** 2\n    updates[accu] = accu_new\n    updates[param] = param - learning_rate * grad / T.sqrt(accu_new + epsilon)\nreturn updates\n"
}